{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d347f210-80bb-4a51-8b4f-5f726bda847d",
   "metadata": {},
   "source": [
    "![logos](media/logos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323caee-c77e-46fe-8dc5-7cc13a67fc21",
   "metadata": {},
   "source": [
    "# Practical Demonstration of New User-Requested Nav2 Features #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137ea0d-f99d-42bd-8d55-3cf876808b6e",
   "metadata": {},
   "source": [
    "## Who Am I?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9637fa-9a7e-4e88-965b-9cc4e51a392d",
   "metadata": {},
   "source": [
    "Hi, I'm [Steve Macenski](https://www.linkedin.com/in/steve-macenski-41a985101/). I do a great deal of different \"robotics stuff\" at [Samsung Research America](https://www.sra.samsung.com/) as the *Open-Source Robotics, Engineering Lead*. I consult with internal Samsung robotics teams on technology and business, work on mobile robot and perception research, and lead the ROS2 mobile robotics ecosystem. I sit on the [ROS2 TSC](https://docs.ros.org/en/foxy/Governance.html) and I am the leader of the Navigation Working Group. \n",
    "\n",
    "Before my current role, I was the leader of the robotics team at [Simbe Robotics](https://www.simberobotics.com/) and worked on NASA's Asteroid Redirect Robotic Mission and RESTORE-L (a  servicing mission to Landsat-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53753bbb-6539-4821-b7fe-2b8d908d8de8",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32018c03-133d-4f25-8520-0dda10c47645",
   "metadata": {},
   "source": [
    "Nav2 is the second generation of the ROS Navigation Stack. It contains a growing set of capabilities, algorithms, and features to build production and research applications. You can find more information about Nav2 at: https://navigation.ros.org.\n",
    "\n",
    "In the following talk, we're going to discuss some of the more basic new features in Nav2 that can help you build your research or applications. This includes:\n",
    "    - Basic Nav2 operation through Rviz2 using **NavigateToPose**\n",
    "    - Basic Nav2 operation through a Python3 script using **NavigateThroughPoses**\n",
    "    - Use of the Waypoint Follower and Task Executor plugins\n",
    "    - Introduction to keepout zones and speed restricted zones\n",
    "\n",
    "We will then put all of these together to show a basic autonomous robotics demo based on Nav2. We will be doing this in a simulated warehouse where robots are often deployed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a8573-5ad1-4e58-8d58-17dd8efd989e",
   "metadata": {},
   "source": [
    "<img src=\"media/warehouse.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "*Taken from [here](https://github.com/aws-robotics/aws-robomaker-small-warehouse-world)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf93a00-0790-488b-885c-38cf1312a8ec",
   "metadata": {},
   "source": [
    "## Nav2 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a13d63-06e8-445b-b06d-aa60a2d0b79a",
   "metadata": {},
   "source": [
    "- Independent, modular servers\n",
    "- Configurable navigation logic, dynamically loaded BT nodes, and per-task behavior selection\n",
    "- A growing set of run-time reconfigurable algorithms and plugins\n",
    "- Software quality, linting, and testing (86% test coverage)\n",
    "- Documentation and a rich community with Slack and Working Group (Join us! [Sign up for the working group here](https://calendar.google.com/calendar/u/0/embed?src=agf3kajirket8khktupm9go748@group.calendar.google.com&ctz=America/Los_Angeles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7f742-5a68-41fc-a5c9-4436c96ccad9",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://navigation.ros.org/_images/architectural_diagram.png\" alt=\"drawing\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205e4b0-47fb-4feb-be3f-5967982f0abc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b029e12-5aab-40a2-a53c-a870b138f193",
   "metadata": {},
   "source": [
    "## Part 1) Let's Get Started!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17a5802e-a8d3-45b2-83c8-e8431a368048",
   "metadata": {},
   "source": [
    "# Lets build our workspace containing the demonstration code\n",
    "source /opt/ros/galactic/setup.bash\n",
    "cd demo_ws/\n",
    "colcon build\n",
    "source demo_ws/install/setup.bash\n",
    "\n",
    "# Now export Gazebo Model Paths for access to all the necessary simulation goodies\n",
    "export GAZEBO_MODEL_PATH=`pwd`/src/turtlebot3_simulations/turtlebot3_gazebo/models:`pwd`/src/aws-robomaker-small-warehouse-world/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb341b4e-abde-4455-a339-66a9c5ec8524",
   "metadata": {},
   "source": [
    "While that's building, lets inspect the `nav2_rosdevday_2021` package a little... You'll find:\n",
    "\n",
    "    - launch: Launch files for the simulation, nav2, rviz, and robot state publisher. It's a one-stop shop for bringup. Setting `use_sim_time:=False` and `use_simulator:=False` allows you to launch with hardware!\n",
    "    - maps: Maps and costmap filter masks for navigation\n",
    "    - params: parameter files for navigation\n",
    "    - worlds: simulation world files containing the robot and warehouse models\n",
    "    - scripts: basic autonomy scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18308057-3154-4d37-bb56-3e2739d02f77",
   "metadata": {},
   "source": [
    "Once your packages have built and are setup, running the command below should launch Nav2, the simulation, and Rviz2:\n",
    "\n",
    "`ros2 launch nav2_rosdevday_2021 system_launch.py`\n",
    "\n",
    "<img src=\"media/initial_view.png\" alt=\"drawing\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60c36c-94e1-41a9-b77a-239166f80af7",
   "metadata": {},
   "source": [
    "Now in your terminal you're probably seeing a bunch of messages pass by, that's OK. We have now yet set our initial position, so the servers are unable to finish the lifecycle initialization to start safe navigation. This is their way of individually complaining until you tell the system roughly where the robot is starting at.\n",
    "\n",
    "You can do this through Rviz2 using **`2D Pose Estimation`** at the robot's position in the world for a demonstration or testing. You can also do this programmatically for a real system which we can mock up via:\n",
    "\n",
    "`ros2 topic pub /initialpose geometry_msgs/msg/PoseWithCovarianceStamped \"{header: {stamp: {sec: 0}, frame_id: 'map'}, pose: {pose: {position: {x: 3.45, y: 2.15, z: 0.0}, orientation: {z: 1.0, w: 0.0}}}}\"`\n",
    "\n",
    "<img src=\"media/pose_set.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "You should now see the robot initialized at its position, the costmaps update in Rviz2, and the warnings should stop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ecd44b-cfc4-4a9e-9cb8-5c6467f935c5",
   "metadata": {},
   "source": [
    "Now, simply use the **`Nav2 Goal`** button to request the robot to go to a specific goal to test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e78375-c3a0-433d-89e8-03085a0b8fe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"test\" loop autoplay width=800>\n",
       "        <source src=\"./media/first_test.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"test\" loop autoplay width=800>\n",
    "        <source src=\"./media/first_test.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d92f50-7a5b-4d22-af8d-281ec584b51c",
   "metadata": {},
   "source": [
    "#### **Now you're navigating!** \n",
    "\n",
    "Play around, get a feel for it. If you find yourself in a bad state at any time during this tutorial, feel free to close out the application and relaunch with the instructions above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4679b-a599-4e8a-8f79-b83bf81f78c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f292ad-2db2-492c-9107-6a6d4f04444d",
   "metadata": {},
   "source": [
    "## Part 2) Navigate Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d965c2c-48d4-402e-8f08-b64a38249be3",
   "metadata": {},
   "source": [
    "There are three primary navigation actions the Nav2 stack exposes currently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244fef2-81b6-46b3-8809-3c0c50cf82ee",
   "metadata": {},
   "source": [
    "### Navigate To Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31af789-f1e0-424d-9572-b26d10a83f6a",
   "metadata": {},
   "source": [
    "Lets demonstrate this using Rviz2 and the Nav2 plugin to walk through a notional **pick and place** autonomy task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526b997-b308-46bd-914d-c71dfe0f9b94",
   "metadata": {},
   "source": [
    "The NavigateToPose action is most suitable for point-to-point navigation requests or for other tasks that can be represented in a behavior tree with a boundary condition pose, such as dynamic object following.\n",
    "\n",
    "NavigateToPose.action:\n",
    "\n",
    "``` bash\n",
    "#goal definition\n",
    "geometry_msgs/PoseStamped pose\n",
    "string behavior_tree\n",
    "---\n",
    "#result definition\n",
    "std_msgs/Empty result\n",
    "---\n",
    "# feedback definition\n",
    "geometry_msgs/PoseStamped current_pose\n",
    "builtin_interfaces/Duration navigation_time\n",
    "builtin_interfaces/Duration estimated_time_remaining\n",
    "int16 number_of_recoveries\n",
    "float32 distance_remaining\n",
    "```\n",
    "\n",
    "As you can see, the action's primary inputs are the `pose` you'd like the robot to navigate to and the (optional) `behavior_tree` to use. If none is specified, it uses the default behavior tree in the BT Navigator. During the action's execution, you'll get feedback with important information like the robot's pose, how much time has elapsed, the estimated time remaining, the distance remaining, and the number of recoveries executed while navigating to the goal. This information can be used to make good autonomy decisions or track progress.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f569230-e4d2-4f99-a5fe-556de7cf5dae",
   "metadata": {},
   "source": [
    "come up with demo around this and explain the rviz plugin along the way TODO\n",
    "\n",
    "go to manual forklift, talk about feedback, notional get load from person or arm, go to empty space, notional unloaded by another person or arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d76b4e-db69-4c94-a258-3f28209ec350",
   "metadata": {},
   "source": [
    "### Navigate Through Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00e69d-926d-4ce5-943d-0729f784880d",
   "metadata": {},
   "source": [
    "Lets demonstrate this using a python3 script to mock up a basic **security** autonomy task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a3370-76ef-413f-bd7c-74d2bd946e6f",
   "metadata": {},
   "source": [
    "The NavigateThroughPoses action is most suitable for pose-constrained navigation requests or for other tasks that can be represented in a behavior tree with a set of poses.\n",
    "\n",
    "NavigateThroughPoses.action:\n",
    "\n",
    "``` bash\n",
    "#goal definition\n",
    "geometry_msgs/PoseStamped[] poses\n",
    "string behavior_tree\n",
    "---\n",
    "#result definition\n",
    "std_msgs/Empty result\n",
    "---\n",
    "#feedback definition\n",
    "geometry_msgs/PoseStamped current_pose\n",
    "builtin_interfaces/Duration navigation_time\n",
    "builtin_interfaces/Duration estimated_time_remaining\n",
    "int16 number_of_recoveries\n",
    "float32 distance_remaining\n",
    "int16 number_of_poses_remaining\n",
    "```\n",
    "\n",
    "As you can see, the action's inputs are nearly identical to NavigateToPose, except now we take in a vector of `poses`. The feedback is similar as well, only containing the new `number_of_poses_remaining` field to track progress through the via-points.\n",
    "\n",
    "come up with demo around this and explain the python script along the way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521247c-79df-4aec-86b8-30350a79581d",
   "metadata": {},
   "source": [
    "come up with demo around this and explain the python script along the way TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c33d2-7e24-4451-a939-38dfc775b71f",
   "metadata": {},
   "source": [
    "### Waypoint Following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43615bb3-08b6-4012-bec9-a13853bd1650",
   "metadata": {},
   "source": [
    "Lets demonstrate this using a python3 script to mock up a basic **inspection** autonomy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043edb9-2152-4663-b317-356c4b1f1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "(explain action, feedback, what this is good for)\n",
    "\n",
    "come up with demo around this and explain the task executors along the way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd80956-690d-43be-8ab1-6ccd37ff7207",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc4c79-33d5-4a81-a666-6902888d5392",
   "metadata": {},
   "source": [
    "## Part 3) Costmap Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca066ba-5e3a-4ae7-8b40-ef86985fb10b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9886fa29-1478-4ea9-b523-65291d1a60ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c68ec92-fe43-4d0e-af61-e934424e026b",
   "metadata": {},
   "source": [
    "## Part 4) Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f4ab9-c4c6-4fb7-b735-38fb4075e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(launch file with keepout, speed zones, in warehouse, doing a task autonomously)\n",
    "\n",
    "video here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdb9b4-4f52-451b-af64-3a676011562a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0f921-abc8-4b46-b2bc-7e61f3cd4971",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce710f3-0c54-4c93-94c5-c88f22bc3ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
